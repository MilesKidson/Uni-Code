\documentclass[12pt]{article}
\usepackage[margin=1.2in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{physics}
\usepackage{tabto}
\usepackage{float}
\usepackage{amssymb}
\usepackage{pgfplots}
\usepackage{verbatim}
\usepackage{tcolorbox}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{siunitx}
\pgfplotsset{compat=1.16}
\definecolor{stringcolor}{HTML}{C792EA}
\definecolor{codeblue}{HTML}{2162DB}
\definecolor{commentcolor}{HTML}{4A6E46}
\lstset{
    basicstyle=\ttfamily\footnotesize,
    commentstyle=\color{commentcolor},
    keywordstyle=\color{codeblue},
    stringstyle=\color{stringcolor},
    showstringspaces=false,
    numbers=left,
    upquote=true,
    captionpos=t,
    abovecaptionskip=12pt,
    belowcaptionskip=12pt,
    language=Python,
    breaklines=true,
    frame=single
    }
\renewcommand{\lstlistingname}{Appendix}

\title{More Advanced Model Fitting and Plotting}
\author{\textbf{PHY2004W \hspace{8cm} KDSMIL001}}
\date{\textbf{24 Feb 2020}}

\begin{document}

    \begin{titlepage}
        \maketitle
        \tableofcontents
    \end{titlepage}

    \section{Answers}
    The first section of the activity was plotting the best-fit curve for a set of non-linear 
    points. The code for everything in this section can be found in Appendix 1.
    Firstly, we were asked to plot the data supplied to us in DampedData.txt, which was a text 
    file containing time and position data of a damped oscillator. To do this we used the errorbar 
    function in \texttt{matplotlib.pyplot} [line 31]. Next, we defined a function that takes in 
    a set of parameters and returns a value for $y(t)$ where

    \begin{equation}
        y(t) = A+Be^{-\gamma t}cos(\omega t-\alpha)
    \end{equation}
    
    \noindent
    This is the equation for the position of a damped oscillator with respect to time. $A, B, 
    \gamma, \omega,$ and $\alpha$ are parameters that change the shape of the curve plotted 
    by this function in various ways. The function defined on line 36 takes these parameters, 
    as well as $t$, and returns a value for the position. \newline
    In order to begin fitting a curve to this data, we first need a set of initial parameters. 
    These are defined on line 20 and were obtained by guessing a few and then adjusting them 
    until we reached a curve that very roughly fit the data points. They are defined in an array 
    in order to be passed to the function that we'll be using later to properly fit the curve 
    to the data. Below, in Figure \ref{fig:Initial Guess}, you can see the curve of our initial 
    guess along with the values of each parameter in Table \ref{table:Initial Params}.
    
    \begin{figure}[H]
        \begin{center}
           \scalebox{.8}{\input{CP2a_Initial_Guess.pgf}}
           \caption{Initial Guess Curve}
           \label{fig:Initial Guess}
        \end{center}
    \end{figure}

    \begin{table}[H]
        \centering
        \begin{tabular}{c c c c c}
            \hline
            A & B & $\gamma$ & $\omega$ & $\alpha$ \\
            \hline
            0.28 & 0.04 & 0.4 & 30 & 0 \\
            \hline
        \end{tabular}
        \caption{Initial Guess Parameters}
        \label{table:Initial Params}
    \end{table}
    
    \noindent
    As you can see, the curve fits reasonably well to the data points. It has roughly the same 
    frequency, initial amplitude, and decay rate, which means it's a good initial guess for our 
    algorithm to start with. 
    \newline
    \newline
    The algorithm we used to fit the function to the set of data points was the Levenberg-Marquardt 
    Algorithm, implemented in the \texttt{scipy.optimize} module, specifically the function 
    \texttt{curve\_fit}. The implementation of this function [lines 47-49] is slightly complex 
    but, after providing it with the function we defined earlier, the data points we are 
    considering, and out initial guesses for the parameters, it returns an array of parameters 
    that it determines to be the optimal parameters to use in order to approximately fit the 
    curve to the data. We then feed these parameters back to our original function and it gives 
    us values for $y(t)$ that are very close to correct. The "goodness" of these values is 
    discussed later on. For now we can have a look at the plot of this curve [Figure \ref{fig:Best Fit}] 
    and see that it seems to be reasonably accurate. 

    \begin{figure}[H]
        \begin{center}
           \scalebox{.8}{\input{CP2a_Best_Fit.pgf}}
           \caption{Algorithmically Determined Best Fit}
           \label{fig:Best Fit}
        \end{center}
    \end{figure}
    
    \newpage\noindent
    As mentioned before, this line of best fit comes with a so-called "goodness" that we call 
    $\chi ^2$ where

    \begin{equation}
        \chi^2=\sum\limits_{i=0}^{n}\frac{y_i-f(t_i,A,B,\gamma,\omega,\alpha)}{u_i}
    \end{equation}

    \noindent
    This $\chi^2$, calculated on lines 52-53, when divided by \texttt{dof}, the "degrees of 
    freedom" of the data (i.e. the number of data points minus the number of fitted parameters 
    [line 54]) gives us a much more useful and convenient measure of the fit. The results of 
    these calculations are below in Table \ref{table:Goodness}.

    \begin{table}[H]
        \centering
        \begin{tabular}{c c c}
            \hline
            $\chi^2$ & \texttt{dof} & $\frac{\chi^2}{\texttt{dof}}$ \\
            \hline
            252.73 & 245 & 1.03 \\
            \hline
        \end{tabular}
        \caption{Goodness Parameters}
        \label{table:Goodness}
    \end{table}

    \noindent
    Our value for $\frac{\chi^2}{\texttt{dof}}$ is $\sim1.03$ and ideally this value is $\sim1$, so our 
    fit is fairly reasonable. 
    \newline
    The next thing to consider is the uncertainties of the actual parameters used in the final 
    fit. These can be extracted from the covariance matrix \texttt{pcov}, which comes out of 
    the calling of \texttt{curve\_fit} in line 47. We introduce a correction factor of 
    $\sqrt{\frac{\chi^2}{\texttt{dof}}}$ as $\frac{\chi^2}{\texttt{dof}}$ deviates from 1. These 
    calculations can be found in lines 62-63, resulting in Table \ref{table:Params and Uncertainties}.

    \begin{table}[H]
        \centering
        \begin{tabular}{c c c c c}
            \hline
            $A$ & $B$ & $\gamma$ & $\omega$ & $\alpha$ \\
            \hline
            0.28 & -0.028 & 0.28 & 21.46 & -2.81 \\
            \num{\pm6.4e-5} & \num{\pm2.47e-4} & \num{\pm4.61e-3} & \num{\pm4.66e-3} & \num{\pm8.87e-3} \\
            \hline
        \end{tabular}
        \caption{Parameter Values}
        \label{table:Params and Uncertainties}
    \end{table}

    \noindent
    The results of line 65, i.e. the uncertainties of the parameters without the correction 
    factor, can be seen below in Table \ref{table:Unweighted Param Uncertainties}.

    \begin{table}[H]
        \centering
        \begin{tabular}{c c c c c}
            \hline
            $u(A)$ & $u(B)$ & $u(\gamma)$ & $u(\omega)$ & $u(\alpha)$ \\
            \hline
            \num{6.33e-5} & \num{2.43e-4} & \num{4.54e-3} & \num{4.59e-3} & \num{8.73e-3} \\
            \hline
        \end{tabular}
        \caption{Uncertainties of Parameters}
        \label{table:Unweighted Param Uncertainties}
    \end{table}

    \noindent
    Finally, we consider the relationship between each parameter and their correlations as these 
    parameters do not exist isolated from everything else. They are all coupled in some way and 
    in order to see the degree to which they are correlated, we calculate the correlation matrix 
    [lines 68-75], displayed below in Table \ref{table:Correlation Matrix}. These values are in 
    the interval [-1, 1] and the matrix is symmetric, so we didn't show all of it. 

    \begin{table}[H]
        \centering
        \begin{tabular}{c c c c c c}
            \hline
             & A & B & $\gamma$ & $\omega$ & $\alpha$ \\
            \hline
            A & 1 \\
            B & 0.00039 & 1 \\
            $\gamma$ & 0.0027 & -0.77 & 1 \\
            $\omega$ & -0.043 & 0.023 & -0.015 & 1 \\
            $\alpha$ & -0.044 & 0.032 & -0.023 & 0.77 & 1 \\
        \end{tabular}
        \caption{Parameter Correlation Matrix}
        \label{table:Correlation Matrix}
    \end{table}

    \noindent
    From this table we can see that the strongest correlations are between $\gamma$ and B, and 
    between $\omega$ and $\alpha$, both of which have a correlation of $\sim0.77$. This is far greater 
    than any other relationship in the system as the rest are an order of magnitude less than 
    these two, at least. These high correlation values are significant when calculating uncertainties 
    as two highly correlated values require a more sophisticated method in order to more reliably 
    calculate their uncertainties. For now we can say that this is a fairly good fit to the data 
    and, excusing the two correlations mentioned earlier, the uncertainties in Table 
    \ref{table:Params and Uncertainties} are valid. 
    \newline
    \newline
    The second section of this activity was an introduction into a weighted linear least-squares 
    fit. The code for all of this is in Appendix 2 and 3.To begin with, we went back to a section 
    of CP1 and replotted the line of best fit for the data in LinearNoErrors.txt using the 
    \texttt{curve\_fit} function and got the result below in Figure \ref{fig:CP1c Plot} as well 
    as the tables below showing the parameters and their uncertainties using the same techniques 
    as for the first analysis. 

    \begin{figure}[H]
        \begin{center}
           \scalebox{.6}{\input{CP1c_Data_Plot.pgf}}
           \caption{Unweighted Linear Least-Squares Fit for LinearNoErrors.txt}
           \label{fig:CP1c Plot}
        \end{center}
    \end{figure}

    \begin{table}[H]
        \begin{minipage}{0.5\textwidth}
            \centering
            \begin{tabular}{c c c}
                \hline
                $\chi^2$ & \texttt{dof} & $\frac{\chi^2}{\texttt{dof}}$ \\
                \hline
                0.99 & 10 & 0.099 \\
                \hline
            \end{tabular}
            \caption{CP1c Goodness Parameters}
            \label{table:CP1c Goodness}
        \end{minipage}
        \begin{minipage}{0.5\textwidth}
            \centering
            \begin{tabular}{c c}
                \hline
                m & c\\
                \hline
                0.59 & 1.07 \\
                $\pm0.026$ & $\pm0.19$ \\
                \hline
            \end{tabular}
            \caption{CP1c Parameter Values}
            \label{table:CP1c Params and Uncertainties}
        \end{minipage}
    \end{table}
        
    \begin{table}[H]
        \begin{minipage}{0.5\textwidth}
            \centering
            \begin{tabular}{c c}
                \hline
                $u(m)$ & $u(c)$ \\
                \hline
                0.084 & 0.62 \\
                \hline
            \end{tabular}
            \caption{CP1c Unweighted Uncertainties of Parameters}
            \label{table:CP1c Unweighted Param Uncertainties}
        \end{minipage}
        \vspace{12pt}
        \begin{minipage}{0.5\textwidth}
            \centering
            \begin{tabular}{c c c}
                \hline
                & m & c \\
                \hline
                m & 1 \\
                c & -0.88 & 1 \\
            \end{tabular}
            \caption{CP1c Parameter Correlation Matrix}
            \label{table:CP1c Correlation Matrix}
        \end{minipage}
    \end{table}

    \begin{table}[H]
        \centering
        \begin{tabular}{cc}
            \hline
            m & c \\
            \hline
            0.59 & 1.07 \\
            $\pm0.026$ & $\pm0.19$ \\
            \hline
        \end{tabular}
        \caption{CP1c Old Uncertainties}
        \label{table:CP1c Old Uncertainties}
    \end{table}

    \noindent
    Looking at the values returned using the new form of analysis, we can see that they are 
    exactly the same as the results printed out in lines 49-52 of Appendix 3 [Table 
    \ref{table:CP1c Old Uncertainties}]. This confirms that \texttt{scipy} correctly calculates 
    uncertainties of parameters if the data has no uncertainty. Regarding the analysis of the 
    parameters, the value for $\frac{\chi^2}{\texttt{dof}}$ is quite different from 1, meaning 
    that the fit is not a very good one. This can be seen again in the uncertainties for m and c 
    in the fact that they are each $\sim10$\% of the value themselves [Table \ref{table:CP1c Params 
    and Uncertainties}]. Even worse is the fact that the correlation between each value is -0.88 
    [Table \ref{table:CP1c Correlation Matrix}], meaning these uncertainties are not as accurate 
    as they should be as a strong correlation between parameters requires more sophisticated 
    methods for calculating uncertainties. In terms of the look of the red line of best fit in 
    Figure \ref{fig:CP1c Plot}, it is exactly the same as the blue plot, our previous plot, 
    which further shows that the methods return the same results. The uncertainties don't seem 
    The code for these calculations is on lines 78-102 of Appendix 3.
    \newline
    \newline
    In order to properly understand what the correlation between parameters means, we use a contour 
    plot, plotting m against c with the corresponding $\frac{\chi^2}{\texttt{dof}}$ for each point. 
    The section of the code that plots the contour is on lines 49-59 of Appendix 2. Below, 
    [Figure \ref{fig:Weighted Linear}] is the actual plot for the weighted linear least-squares 
    fit, along with the contour plot corresponding to it [Figure \ref{fig:Weighted Contour}].
    
    \begin{figure}[H]
        \begin{center}
            \scalebox{.7}{\input{CP2b_Data_Plot.pgf}}
            \caption{Weighted Linear Least-Squares Fit for LinearWithErrors.txt}
            \label{fig:Weighted Linear}
        \end{center}
    \end{figure}
    
    \begin{figure}[H]
        \begin{center}
            \scalebox{.7}{\input{CP2b_Contour_Plot.pgf}}
            \caption{Weighted Contour Plot for LinearWithErrors.txt}
            \label{fig:Weighted Contour}
        \end{center}
    \end{figure}
    
    \newpage\noindent
    Lastly, in Figure \ref{fig:CP1c_Contour_Plot} we have a contour plot that corresponds to 
    the data from LinearNoErrors.txt. It's clear that the range of possible values of 
    $\frac{\chi^2}{\texttt{dof}}$ for the weighted fit is much larger than for the unweighted fit. 
    We've plotted the two contours using the same scale for the value of $\frac{\chi^2}{\texttt{dof}}$ 
    in order to show just how much bigger the range is for the weighted fit. 

    \begin{figure}[H]
        \begin{center}
           \scalebox{.7}{\input{CP1c_Contour_Plot.pgf}}
           \caption{CP1c Unweighted Contour for LinearNoErrors.txt}
           \label{fig:CP1c_Contour_Plot}
        \end{center}
    \end{figure}
    
    \noindent
    The values for the unweighted fit are $\sim0$ for most values considered and $\sim1$ for 
    two small sections in the top right and bottom left. The ideal value for 
    $\frac{\chi^2}{\texttt{dof}}$ is 1 and so we can see that an unweighted fit gives us a 
    much more accurate fit, but in reality data without errors is impossible and so all fits 
    should be weighted. Thus, when fitting a line to some data, it is vital to properly consider 
    the uncertainties of everything, including all the parameters and the correlations between 
    them, in order to be confident that the results are the actual results, even if the so-called 
    "goodness" factor is less than ideal. 
    
    \newpage
    \section{Appendix}
    \lstinputlisting[caption=CP2a\_Nonlinear\_Fitting.py]{CP2a_Nonlinear_Fitting.py}
    \lstinputlisting[caption=CP2b\_Visualising\_Uncertainties.py]{CP2b_Visualising_Uncertainties.py}
    \lstinputlisting[caption=CP1c.py]{CP1c.py}


\end{document}